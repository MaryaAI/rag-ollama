# indexer.py - ingest -> chunk -> embed -> store (pgvector)
import os, pathlib, hashlib, json, logging
from typing import List, Optional, Dict
from dotenv import load_dotenv
from tqdm import tqdm
from docling import DocumentConverter
from llama_index.core import Document, StorageContext, VectorStoreIndex
from llama_index.core.node_parser import SentenceWindowNodeParser
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.embeddings.ollama import OllamaEmbedding

logging.basicConfig(level=logging.INFO)
log = logging.getLogger("indexer")
load_dotenv()

def env(k: str, d: Optional[str]=None) -> str:
    v = os.getenv(k, d)
    if v is None:
        raise RuntimeError(f"Missing env var: {k}")
    return v

def chunk_hash(text: str, metadata: Dict) -> str:
    h = hashlib.sha256()
    h.update(text.encode("utf-8"))
    meta_json = json.dumps(metadata, sort_keys=True, ensure_ascii=False).encode("utf-8")
    h.update(meta_json)
    return h.hexdigest()

def load_docs(data_dir: str = "data") -> List[Document]:
    data_path = pathlib.Path(data_dir)
    if not data_path.exists():
        raise FileNotFoundError(f"data dir not found: {data_dir}")
    conv = DocumentConverter()
    docs: List[Document] = []
    files = [p for p in data_path.rglob("*") if p.is_file()]
    log.info(f"Found {len(files)} files to convert in {data_dir}")
    for p in tqdm(files, desc="Converting docs"):
        try:
            r = conv.convert(str(p))
            docs.append(Document(text=r.text, metadata={"source": str(p.resolve()), "filename": p.name}))
        except Exception as e:
            log.warning(f"Failed to parse {p}: {e}")
    return docs

def build_or_update_index(docs: List[Document]):
    chunk_size = int(env("CHUNK_SIZE", "700"))
    chunk_overlap = int(env("CHUNK_OVERLAP", "120"))
    parser = SentenceWindowNodeParser.from_defaults(chunk_size=chunk_size, chunk_overlap=chunk_overlap, window_size=2)
    log.info(f"Parsing documents into nodes (chunk_size={chunk_size}, overlap={chunk_overlap})")
    nodes = parser.get_nodes_from_documents(docs)

    normalized_nodes = []
    for n in nodes:
        text = n.get_text()
        md = n.metadata or {}
        normalized_meta = {"source": md.get("source"), "filename": md.get("filename"), "extra": md.get("extra", {})}
        cid = chunk_hash(text, normalized_meta)
        normalized_meta["chunk_id"] = cid
        n.metadata = normalized_meta
        normalized_nodes.append(n)

    embed_model_name = env("EMBED_MODEL", "all-minilm")
    embed_dim = int(env("EMBED_DIM", "384"))
    embed = OllamaEmbedding(model_name=embed_model_name, base_url=env("OLLAMA_BASE_URL", "http://localhost:11434"))

    vs = PGVectorStore.from_params(
        database=env("PGDATABASE"),
        host=env("PGHOST"),
        password=env("PGPASSWORD"),
        port=int(env("PGPORT")),
        user=env("PGUSER"),
        table_name=env("PG_TABLE", "rag_chunks"),
        embed_dim=embed_dim,
    )
    storage = StorageContext.from_defaults(vector_store=vs)
    log.info("Building/persisting index (this may take time)...")
    index = VectorStoreIndex(normalized_nodes, storage_context=storage, embed_model=embed)
    log.info("✅ Index build finished and persisted.")
    return index

def main():
    docs = load_docs("data")
    if not docs:
        log.error("No documents found in ./data — add files and retry.")
        return
    build_or_update_index(docs)

if __name__ == "__main__":
    main()
