# backend_server.py - minimal FastAPI service (retrieval + Ollama shim)
import os, json
from typing import Optional, List, Dict, Any
from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import requests
from llama_index.llms.ollama import Ollama
import re
import time
from llama_index.vector_stores.postgres import PGVectorStore
from llama_index.core import VectorStoreIndex, StorageContext
from llama_index.embeddings.ollama import OllamaEmbedding
from llama_index.llms.ollama import Ollama

load_dotenv()
def env(k,d=None): v=os.getenv(k,d); 
def_env_missing=False
if env("__DUMMY__",None): pass
# Simple env helper
def env_func(k,d=None):
    v=os.getenv(k,d)
    if v is None:
        raise RuntimeError(f"Missing env var: {k}")
    return v
OLLAMA_BASE = os.getenv("OLLAMA_BASE_URL","http://localhost:11434")
EMBED_MODEL = os.getenv("EMBED_MODEL","all-minilm")
EMBED_DIM = int(os.getenv("EMBED_DIM","384"))


def ollama_generate(payload: dict) -> dict:
    """
    Call Ollama /api/generate and return parsed JSON response.
    Raises HTTPException (502) on failure so FastAPI returns an error.
    """
    try:
        resp = requests.post(f"{OLLAMA_BASE}/api/generate", json=payload, timeout=120)
        resp.raise_for_status()
        return resp.json()
    except Exception as e:
        # wrap to HTTPException so it surfaces in API response
        raise HTTPException(status_code=502, detail=f"Ollama generate error: {e}")



PG_PARAMS = dict(
    database=os.getenv("PGDATABASE","ragdb"),
    host=os.getenv("PGHOST","localhost"),
    password=os.getenv("PGPASSWORD","postgres"),
    port=int(os.getenv("PGPORT","5432")),
    user=os.getenv("PGUSER","postgres"),
    table_name=os.getenv("PG_TABLE","rag_chunks"),
    embed_dim=EMBED_DIM,
)

app = FastAPI(title="RAG backend")

INDEX=None
def get_index():
    global INDEX
    if INDEX is not None:
        return INDEX
    vs = PGVectorStore.from_params(**PG_PARAMS)
    storage = StorageContext.from_defaults(vector_store=vs)
    INDEX = VectorStoreIndex.from_vector_store(vs, storage_context=storage, embed_model=OllamaEmbedding(model_name=EMBED_MODEL, base_url=OLLAMA_BASE))
    return INDEX

@app.get("/healthz")
def healthz():
    return {"ok": True, "ollama": OLLAMA_BASE}

class RagQuery(BaseModel):
    query: str
    top_k: Optional[int] = 5

# @app.post("/rag/query")
# def rag_query(body: RagQuery):
#     idx = get_index()
#     try:
#         qe = idx.as_query_engine(similarity_top_k=body.top_k or 5)
#         answer = qe.query(body.query)
#         return {"answer": str(answer), "raw": answer}
#     except Exception as e:
#         raise HTTPException(status_code=500, detail=str(e))


def _parse_score_from_text(text: str):
    """
    Try to extract a numeric score from Ollama output.
    Accepts JSON-like or plain numbers embedded in text.
    """
    if not text:
        return None
    # try JSON
    try:
        import json
        j = json.loads(text)
        if isinstance(j, dict) and "score" in j:
            return float(j["score"])
    except Exception:
        pass
    # fallback: find first float-looking substring
    m = re.search(r"([0-9]*\.?[0-9]+)", text)
    if m:
        try:
            v = float(m.group(1))
            # clamp
            return max(0.0, min(1.0, v))
        except Exception:
            return None
    return None

def _score_candidates_with_ollama(candidates, query, model=None, base_url=None, timeout=30.0):
    """
    candidates: list of dicts { 'text_preview': str, 'source': ..., 'filename': ..., 'chunk_id': ... }
    returns: list of tuples (candidate, score) with score in 0..1
    Implementation: call Ollama /api/chat per candidate with a small scoring prompt (could be batched)
    """
    scores = []
    model = model or os.getenv("GEN_MODEL", "smollm:135m")
    base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")

    for c in candidates:
        # build minimal prompt instructing model to output JSON {"score":0.0}
        prompt = (
            "You are a relevance scorer. Given the user query and a document excerpt, "
            "return a JSON object with a single numeric field 'score' with a value between 0 and 1 "
            "representing relevance (1 = highly relevant). No other text.\n\n"
            f"User query: \"{query}\"\n\n"
            "Document excerpt:\n"
            f"\"\"\"\n{c.get('text_preview','')[:1500]}\n\"\"\"\n\n"
            "Return only JSON like: {\"score\":0.87}"
        )

        payload = {"model": model, "messages": [{"role":"user", "content": prompt}], "stream": False}
        try:
            resp = requests.post(f"{base_url}/api/chat", json=payload, timeout=timeout)
            resp.raise_for_status()
            body = resp.json()
            # body shape may vary: try to extract assistant content
            content = None
            if isinstance(body, dict):
                # try a few shapes
                if "content" in body:
                    content = body.get("content")
                elif "responses" in body and isinstance(body["responses"], list) and body["responses"]:
                    first = body["responses"][0]
                    if isinstance(first, dict):
                        content = first.get("content") or first.get("message") or str(first)
                    else:
                        content = str(first)
                else:
                    content = str(body)
            else:
                content = str(body)
            score = _parse_score_from_text(content)
            if score is None:
                # fallback: very small default to avoid crash
                score = 0.0
        except Exception as e:
            # on failure, set score 0 and continue
            score = 0.0
        scores.append((c, float(score)))
    return scores

@app.post("/rag/query")
def rag_query(body: RagQuery):
    """
    RAG with re-ranking + synthesizing on top contexts.
    Returns final synthesized answer plus reordered sources with scores.
    """
    index = get_index()
    try:
        # create Ollama LLM for generation
        llm = Ollama(model=os.getenv("GEN_MODEL", "smollm:135m"),
                     base_url=os.getenv("OLLAMA_BASE_URL"),
                     request_timeout=120.0)

        # Step A: retrieve a broader set of candidates
        retrieval_k = max(5, (body.top_k or 5) * 3)
        qe = index.as_query_engine(similarity_top_k=retrieval_k, llm=llm)
        response = qe.query(body.query)

        # Best-effort extract candidate source_nodes
        candidates = []
        try:
            if hasattr(response, "source_nodes") and response.source_nodes:
                for sn in response.source_nodes:
                    node = getattr(sn, "node", None) or getattr(sn, "source_node", None) or getattr(sn, "source", None)
                    score = getattr(sn, "score", None)
                    md = {}
                    txt = None
                    try:
                        if node is not None:
                            txt = node.get_text()[:2000] if hasattr(node, "get_text") else None
                            md = getattr(node, "metadata", {}) or {}
                    except Exception:
                        md = {}
                    candidates.append({
                        "chunk_id": md.get("chunk_id"),
                        "source": md.get("source"),
                        "filename": md.get("filename"),
                        "text_preview": txt,
                        "orig_score": float(score) if score is not None else None
                    })
            elif hasattr(response, "get_formatted_sources"):
                # fallback: when we only get formatted sources
                fmt = response.get_formatted_sources()
                candidates = [{"chunk_id": None, "source": None, "filename": None, "text_preview": fmt, "orig_score": None}]
            elif isinstance(response, dict) and "sources" in response:
                # if the response is dict-like
                for s in response.get("sources", []):
                    candidates.append({"chunk_id": s.get("chunk_id"), "source": s.get("source"), "filename": s.get("filename"), "text_preview": s.get("text")[:2000], "orig_score": s.get("score")})
        except Exception:
            candidates = []

        if not candidates:
            # fallback: return the original response string and empty sources
            return {"answer": str(response), "sources": [], "raw": str(response)}

        # Step B: rerank candidates using Ollama-based cross-encoder (scores 0..1)
        scored = _score_candidates_with_ollama(candidates, body.query,
                                               model=os.getenv("GEN_MODEL", "smollm:135m"),
                                               base_url=os.getenv("OLLAMA_BASE_URL", "http://localhost:11434"))
        # sort descending by score
        scored_sorted = sorted(scored, key=lambda t: t[1], reverse=True)

        # choose final top_k
        final_k = body.top_k or 5
        top_scored = scored_sorted[:final_k]

        # Step C: synthesize final answer conditioned on the top contexts (explicit prompt)
        contexts_text = ""
        for i, (cand, sc) in enumerate(top_scored, start=1):
            contexts_text += f"CONTEXT {i} (score={sc:.3f}) SOURCE: {cand.get('source')}\n{cand.get('text_preview','')}\n\n---\n\n"

        synth_prompt = (
            "You are a helpful assistant that must answer the user's query using ONLY the provided contexts. "
            "If the answer is not in the contexts, say 'I don't know' (do not hallucinate). "
            "Cite the source path after each sentence or bullet where relevant.\n\n"
            f"User query: {body.query}\n\n"
            "Contexts:\n\n" + contexts_text +
            "\n\nTask: Provide a concise answer (3-6 lines) grounded in the contexts and list sources used.\n"
        )

        # use Ollama generate API to synthesize final answer
        synth_payload = {"model": os.getenv("GEN_MODEL", "smollm:135m"), "prompt": synth_prompt}
        synth_resp = ollama_generate(synth_payload)
        # synth_resp may be structured variously; try to extract content
        synth_text = None
        if isinstance(synth_resp, dict):
            # try common keys
            synth_text = synth_resp.get("content") or synth_resp.get("output") or (synth_resp.get("responses", [{}])[0].get("content") if synth_resp.get("responses") else None)
            if synth_text is None:
                synth_text = str(synth_resp)
        else:
            synth_text = str(synth_resp)

        # prepare sources array for return (include model's rerank score)
        out_sources = []
        for cand, sc in scored_sorted[:final_k]:
            out_sources.append({
                "chunk_id": cand.get("chunk_id"),
                "source": cand.get("source"),
                "filename": cand.get("filename"),
                "score": float(sc),
                "text_preview": cand.get("text_preview")[:400] if cand.get("text_preview") else None
            })

        return {"answer": synth_text, "sources": out_sources, "raw_retrieval": str(response)}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Retrieval/Rerank error: {e}")
